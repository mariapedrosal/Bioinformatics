{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatics Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Cancer classification based on molecular level investigation has gained the interest of researches as it provides a systematic, accurate and objective diagnosis for different cancer types. In this project, we introduce an approach for classifying two different kind of leukaemia based on gene expression profiles. In order to perfom this classification, we applied seven machine learning (ML) algorithms, hoping to get an accurate model that could be used to predict the type of cancer of any patiend based on its expression profile. For this aim, we compared the different classification models basing our conclusions in the AUROC obtained for it. We considered that an AUROC over 0.95 would point to an acceptable model. \n",
    "\n",
    "\n",
    "## Preliminar data analysis\n",
    "\n",
    "Our available data for the construction of prediction models consistd in two dataframes. The first one, already labelled as training data, contained 38 observations of patients, to whom the expression of 7129 genes had been assessed in order to identify any characteristic expression pattern for differential diagnosis of two leukaemia conditios: acute myeloid leukaemia (AML) and acute lymphoid leukaemia (ALL). The test data (in which the classification models are going to be tested) consisted in the expression levels of the same genes for 34 different pacients. The actual diagnosis of both groups was also provided.\n",
    "\n",
    "## Data pre-processing\n",
    "Due to the vast extension of the data, an initial pre-procesing was needed in order to minimize the number of features to be used with the ML tools.\n",
    "Since the expression levels of many genes had been analysed, it is important to determine which of them show a correlation with the differential diagnosis of the diseases (i.e. which of them show a significant change in the expression when the patient has been diagnosed with ALL or AML). In order to do that, it is useful to perfom a feature selection.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maria\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#Importing all the necessary libraries for the ML analysis\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the files:\n",
    "\n",
    "#These paths might change according to the locations of the files within one's computer\n",
    "ruta_train='C:\\\\Users\\\\Maria\\\\OneDrive\\\\Master\\\\Bioinformatica\\\\Trabajo\\\\Originales\\\\data_set_ALL_AML_train.csv'\n",
    "ruta_y='C:\\\\Users\\\\Maria\\\\OneDrive\\\\Master\\\\Bioinformatica\\\\Trabajo\\\\Originales\\\\actual.csv'\n",
    "ruta_test='C:\\\\Users\\\\Maria\\\\OneDrive\\\\Master\\\\Bioinformatica\\\\Trabajo\\\\Originales\\\\data_set_ALL_AML_independent.csv'\n",
    "train=pd.read_csv(ruta_train)\n",
    "test=pd.read_csv(ruta_test)\n",
    "y=pd.read_csv(ruta_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features were initially placed in the rows of the dataframe, while patients were in the columns. Thus, it was necessary to transpose the data. Besides, there were some columns name 'call', between the patients information, whose data was not relevant for our model. These columns were also removed. Lastly, the name of the features was changed and the actual expression measure was converted into a numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transposing the data\n",
    "train=train.T\n",
    "test=test.T\n",
    "\n",
    "\n",
    "#Removing call data\n",
    "for fila in train.index:\n",
    "    if 'call' in fila:\n",
    "        train=train.drop(fila)\n",
    "\n",
    "for fila in test.index:\n",
    "    if 'call' in fila:\n",
    "        test=test.drop(fila)\n",
    "\n",
    "#Columns are labelled with the gene accession number\n",
    "\n",
    "columnastrain=train.loc['Gene Accession Number']\n",
    "train=train[2:]\n",
    "train.columns=columnastrain\n",
    "\n",
    "columnastest=test.loc['Gene Accession Number']\n",
    "test=test[2:]\n",
    "test.columns=columnastest\n",
    "\n",
    "#Converting into numeric\n",
    "train=train.astype('float')\n",
    "test=test.astype('float')\n",
    "train.index=train.index.astype(int)\n",
    "test.index=test.index.astype(int)\n",
    "train=train.sort_index()\n",
    "test=test.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response was coded as a categorical variable with 1 coding ALL and 0 coding AML. This response was split into the diagnosis of test and training patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.index=y['patient']\n",
    "y['cancer']=np.where(y.cancer=='ALL', 1, 0) \n",
    "y.groupby('cancer').size() \n",
    "ytrain=y[:len(train)]\n",
    "ytest=y[len(train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data had the accurate structure, it was time to perform the aforementioned feature selection. Two algorithms for this aim were used in a combined way. In the first one, an univariant selection was performed, and thus each feature was individually selected or removed for the final analysis. In order to do that, the Fischer score was computed and the 100 best features were selected. \n",
    "On the other hand, a packing selection tool was used. This type of tools consider the selection as a searching problem (typical of the artificial intelligence tools), and different combinations of features are evaluated and compared. To each of these combinations, a score is computed and assigned and some algorithms are run to select the best combinations. In this case, the algorithm for the selection was the recursive feature removal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected features are: \n",
      "['L38608_at', 'U12471_cds1_at', 'Z68193_at', 'X58401_at', 'J03801_f_at', 'Z86000_at', 'X67698_at', 'M33317_f_at', 'M63138_at', 'HG4535-HT4940_s_at', 'X00038_at', 'D38498_f_at', 'X98296_at', 'X72475_at', 'J04027_at', 'X06985_at', 'M95178_at', 'HG4490-HT4876_f_at', 'D64053_at', 'Z83336_at', 'K03189_f_at', 'L08246_at', 'U61836_at', 'M11147_at', 'M27318_f_at', 'M21551_rna1_at', 'Z80781_at', 'L05188_f_at', 'M62762_at', 'L42583_f_at', 'M92269_f_at', 'L42611_f_at', 'Y00339_s_at', 'M32304_s_at', 'U10690_f_at', 'HG458-HT458_f_at', 'U41767_s_at', 'Z68274_at', 'V00551_f_at', 'L42379_at', 'AFFX-HUMTFRR/M11507_M_at', 'U07132_at', 'Y00787_s_at', 'X85116_rna1_s_at', 'X69654_at', 'Z48501_s_at', 'M81933_at', 'Z49105_at', 'M16653_at', 'M15395_at', 'X05345_at', 'Z32765_at', 'X58399_at', 'X06825_at', 'M95678_at', 'D10495_at', 'D32129_f_at', 'HG4236-HT4506_f_at', 'D38437_f_at', 'L00389_f_at', 'D14874_at', 'HG67-HT67_f_at', 'M25667_at', 'M81695_s_at', 'X14008_rna1_f_at', 'M68891_at', 'J00148_cds2_f_at', 'X01703_at', 'U82759_at', 'X17042_at', 'M23197_at', 'M27749_r_at', 'Z46261_at', 'M75715_s_at', 'Y13618_at', 'X04085_rna1_at', 'X78712_at', 'M19045_f_at', 'X06318_at', 'L08177_at', 'M21904_at', 'U69611_at', 'M19507_at', 'Z30643_at', 'HG627-HT5097_s_at', 'U67963_at', 'HG4321-HT4591_at', 'M55150_at', 'K03192_f_at', 'M77481_rna1_f_at', 'Y07566_at', 'D38128_at', 'K02405_f_at', 'U22028_at', 'M57423_f_at', 'Z83735_at', 'M83652_s_at', 'HG1515-HT1515_f_at', 'K03183_f_at', 'D17427_at', 'M22324_at', 'M28130_rna1_s_at', 'L18920_f_at', 'U21689_at', 'D17547_at', 'V00532_rna1_f_at', 'D43682_s_at', 'X62654_rna1_at', 'U46751_at', 'L18877_f_at', 'X64364_at', 'M98399_s_at', 'X75042_at', 'X52056_at', 'D49950_at', 'V00533_rna1_f_at', 'AJ000480_at', 'Z30644_at', 'M22612_f_at', 'X61587_at', 'V01516_f_at', 'K03204_f_at', 'M28170_at', 'L76568_xpt3_f_at', 'M83667_rna1_s_at', 'HG1879-HT1919_at', 'J03071_cds3_f_at', 'Z00010_at', 'M57710_at', 'X52142_at', 'HG3707-HT3922_f_at', 'HG2915-HT3059_f_at', 'U03735_f_at', 'X64072_s_at', 'K03207_f_at', 'X97261_r_at', 'HG2139-HT2208_f_at', 'X70297_at', 'L32961_at', 'HG2981-HT3127_s_at', 'U22376_cds2_s_at', 'U65918_f_at', 'D29992_at', 'X58431_rna2_s_at', 'M96326_rna1_at', 'M24069_at', 'L37112_at', 'M22960_at', 'X07743_at', 'X89101_s_at', 'HG3527-HT3721_f_at', 'X69115_at', 'X15414_at', 'J00209_f_at', 'M37435_at', 'M17252_at', 'M16038_at', 'M84526_at', 'M80254_at', 'D86096_cds6_at', 'HG3494-HT3688_at', 'L13278_at', 'V00542_f_at', 'M25296_at', 'M83221_at', 'M33600_f_at', 'M27891_at', 'U50136_rna1_at', 'J00117_f_at', 'L19779_at', 'M60750_f_at', 'S82185_at', 'U10689_f_at', 'M69043_at', 'M86406_at', 'M28209_at', 'D26308_at', 'X98225_at', 'X02958_at', 'X16546_at', 'L20941_at', 'Y12670_at', 'U35234_at', 'M31158_at', 'X72304_at', 'X97261_at', 'L09209_s_at', 'X95735_at', 'M31211_s_at', 'U22029_f_at', 'X83490_s_at', 'D26156_s_at', 'X07730_at', 'U37055_rna1_s_at', 'HG4027-HT4297_f_at', 'L35594_at', 'X59417_at', 'L02326_f_at']\n"
     ]
    }
   ],
   "source": [
    "#Univariant feature selection (F-score)\n",
    "k = 100  \n",
    "columnas = list(train.columns.values)\n",
    "seleccionadas = SelectKBest(f_classif, k=k).fit(train, ytrain['cancer'])\n",
    "atrib = seleccionadas.get_support()\n",
    "atributos1 = [columnas[i] for i in list(atrib.nonzero()[0])]\n",
    "\n",
    "#Recursive atribute selecion\n",
    "modelo = ExtraTreesClassifier()\n",
    "era = RFE(modelo, 100)  # número de atributos a seleccionar\n",
    "era = era.fit(train, ytrain['cancer'])\n",
    "atrib2 = era.support_\n",
    "atributos2 = [columnas[i] for i in list(atrib2.nonzero()[0])]\n",
    "\n",
    "#Se combinan los tributos elegidos en una lista\n",
    "print('The selected features are: ')\n",
    "atribselec=list(set(atributos1)|set(atributos2))\n",
    "print(atribselec)\n",
    "\n",
    "trainred=pd.DataFrame()\n",
    "for i in atribselec:\n",
    "    trainred[i]=train[i]\n",
    "\n",
    "testred=pd.DataFrame()\n",
    "for i in atribselec:\n",
    "    testred[i]=test[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 100 features have been selected with each type of tool, a maximum number of 200 were considered to be significant for our classification models. They may also be too many, and so a principal component analysis (PCA) was performed. This is a statistical tool used for describing complex data in terms of new uncorrelationated variables. In this case, the algorithm is computed in a way that the new variables maintain the 95% of the variance in the original variables. Since PCA is affected by the data scale, it is necessary to standardize it previously to the computing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "              0             1             2             3             4   \\\n",
      "0   -9477.730147   6823.799853   1499.992484  -1247.157021  -2117.343809   \n",
      "1    3860.253252   2159.062917  -6003.545821  -1273.306552   2112.935590   \n",
      "2  -10202.696023  -4634.888097    179.815845  -1367.887901  -3632.324451   \n",
      "3  -10008.308162  -1166.592773    117.062052  -1461.063464  -1847.814948   \n",
      "4   -6611.865752  -1434.682014  -1003.275704   5073.741403   1265.571948   \n",
      "5  -10028.082284  -6438.665438  -1254.438512   1858.443033  -4387.208581   \n",
      "6  -10752.411758  -2675.712812   2044.025572   -736.611404   -736.397704   \n",
      "7   -9370.232779   1509.391160    155.959521  -1604.443895  -2196.110746   \n",
      "8   -6835.684453  -2291.819569    944.069188  -6393.007560  -6545.207336   \n",
      "9  -10820.051867  -5852.197335   -564.215284   1437.632978   1017.574243   \n",
      "10 -11039.280946  -5126.118820  -2321.888609  -1297.536407  -1326.267019   \n",
      "11   1121.881866  -1804.393240   -164.490087  -2113.612497   3058.803158   \n",
      "12  -7966.763037  14785.546443  -2393.889222   3853.691351   3068.218150   \n",
      "13  -4409.002299     64.837663   -496.711472   -986.384137  -4101.202138   \n",
      "14  -6132.986593  10829.241895    334.010717  -3383.325801  -1461.241368   \n",
      "15 -13502.173618  -4119.982303    -90.280623   2853.902326     52.367961   \n",
      "16  -3353.491142   4029.839930   3923.797784  -8697.279266  -4954.075918   \n",
      "17  -8869.825737   6651.978537  -3313.518798   4626.256828    395.742277   \n",
      "18 -12950.857512   2627.320656  -2921.973714   6069.038446   2430.802491   \n",
      "19  -3426.022835  12732.100529  -2153.992420  -4388.664278   1205.301787   \n",
      "20 -14500.694449  -3524.009316   1170.687407  -1522.730887   -746.733997   \n",
      "21  -9811.501091   4534.587409  -3845.408194   4730.737266   7147.195243   \n",
      "22 -13901.163495  -7694.958189    196.356306    713.659464  -2534.028523   \n",
      "23  -9686.716974   -850.712288   5033.709906  -6890.805252  -2982.268955   \n",
      "24   3036.588253   1020.989483  -4949.392618  -4477.869603   8662.124643   \n",
      "25  -9539.997916   7829.169688    832.853353    723.152244   3274.599011   \n",
      "26 -13637.229480  -4928.733132  -2062.507021   4592.213922   -120.888491   \n",
      "27  16358.616821   1023.122986  -7498.409592 -10060.898464   2467.715405   \n",
      "28   8345.575166  -2413.239830  17571.428909    784.879765   3352.765817   \n",
      "29  30718.538613   8560.791131  10575.344947   4841.635318   5556.430524   \n",
      "30   8443.987830  -3503.955876   5458.952617  11612.137814  -3126.900239   \n",
      "31  19808.393751   2331.394730  -2241.212361  -4677.353798  -7925.466461   \n",
      "32  31229.658065  -7528.601482 -13717.557775   2062.929710  -3150.211038   \n",
      "33  14556.191241  -5577.268466  15296.704129  -4776.122869   2478.446274   \n",
      "34  -6469.330847  -5683.900846   -495.905280  11111.877963    467.310565   \n",
      "35  45544.025061   6606.930929  -2836.999703   6850.230940  -8960.613249   \n",
      "36  37647.112650  -5904.567042    305.782258  -1035.356705   1987.328721   \n",
      "37  12633.278628 -10965.107072  -5310.940186  -5404.743006  12851.071162   \n",
      "\n",
      "              5            6            7            8            9   \\\n",
      "0    3674.734570  2089.851772  4830.619725 -4150.487380  1945.771493   \n",
      "1   -4281.762833  -631.792850 -4510.020849  4950.372800  1354.477042   \n",
      "2   -2253.359986 -2332.430099  -502.415909  -577.693992 -1198.166352   \n",
      "3    -458.704043    17.472432  -957.294837  -376.685592   979.701266   \n",
      "4   -1701.276797 -1879.237900 -2434.730560  1845.916027 -1729.579304   \n",
      "5   -1689.508661 -2103.375503 -1704.059567 -1357.408566   949.416174   \n",
      "6    1238.006130 -1283.853020   158.292678 -1339.510878  -162.306913   \n",
      "7   -2227.311323  1902.716497  3409.982003  1438.616545  3053.055169   \n",
      "8   -3431.681872    26.796656 -1881.722291    31.197237 -3109.076841   \n",
      "9     387.858938 -5524.399031 -2365.153617 -1570.866010 -1146.981309   \n",
      "10  -1985.541682 -2648.629110 -2749.226785   178.783947 -3224.869015   \n",
      "11   4500.204497 -1704.186277   -68.021116  4153.965050  4425.736429   \n",
      "12   2059.295153   443.027274  2062.101422  -619.599023 -2302.422667   \n",
      "13  -6143.022686  2907.652751 -3434.797301  5085.230092  4461.208553   \n",
      "14  -3194.183255  2896.336609  -407.017377  2182.512221 -2092.128664   \n",
      "15   1877.082416 -1604.223244    -2.227111 -2130.885415 -2349.664680   \n",
      "16  -2895.179401  5159.360414 -5109.281270  1317.328853  1139.317171   \n",
      "17   3158.874254  2342.903677  3859.435456 -1978.980947  3645.143090   \n",
      "18    553.433700 -1126.600505  2193.077060 -2536.975185 -1341.917186   \n",
      "19  -1568.414280  3728.418558 -2594.927989 -4114.405369 -3968.268800   \n",
      "20   1620.495064 -5689.203095  3171.460646  -267.227612 -6105.468074   \n",
      "21  -4837.228389 -1488.616732  1578.286246  2896.946569  2089.565280   \n",
      "22   -692.973228 -2471.751538  1820.920372 -1357.005706  1376.409009   \n",
      "23   2374.268885  1016.883652  -699.673507 -1786.449779  -319.771800   \n",
      "24   5328.967500 -4717.824375 -1597.240523  7742.882304   556.256431   \n",
      "25    565.997767 -1110.350862  3288.339586  1725.553856  1303.125278   \n",
      "26   -393.791812 -3214.234068  1386.924586  -475.299659  2922.698648   \n",
      "27   6171.639940  1880.393969  3195.504713  -329.902229 -2806.658355   \n",
      "28  -8416.326849  3725.314398  9028.158823  1272.433314 -1548.039163   \n",
      "29  -1011.755604 -6010.921518 -8240.944585 -6008.750793 -1286.062832   \n",
      "30   4334.588232  5095.843777 -3881.905431  1356.667002  -684.296346   \n",
      "31   5382.551571 -3262.441716   414.555782 -5916.078162  7015.379738   \n",
      "32    538.309801  5258.772008  3681.443748  1123.698871 -6023.570039   \n",
      "33  10063.228638  1345.867954  -662.030670  4855.701122 -1066.717467   \n",
      "34   5028.931666  9724.836714 -2542.473307  -228.713521  1013.172698   \n",
      "35  -2025.915175 -3344.046991  -298.729992  3122.366875  -558.008220   \n",
      "36  -5165.442703 -3504.594638  6406.807068 -1373.964296  2489.436049   \n",
      "37  -4485.088143  6090.263961 -3842.015321 -6783.282571  2304.104510   \n",
      "\n",
      "             10           11           12  \n",
      "0  -1600.547404   312.565700   484.794273  \n",
      "1   2028.316028  -533.095168  2917.484060  \n",
      "2   -498.881357  1632.985558 -1673.974400  \n",
      "3  -2150.786212  2062.125589  -947.635414  \n",
      "4  -1674.382146   349.674763  4608.901952  \n",
      "5  -3499.470528  1441.068788   437.961177  \n",
      "6  -1516.399113  1943.877410 -3173.102057  \n",
      "7    911.773192   220.037344 -1967.476075  \n",
      "8   2363.007288  -710.811275   350.436691  \n",
      "9   1523.715750   -44.885481  1915.820138  \n",
      "10  -702.143306  -275.664053  1108.502245  \n",
      "11  1737.162646   130.847249  4463.478267  \n",
      "12 -1104.767089  1917.417545  -388.957521  \n",
      "13 -2610.082974    21.947597  2355.204833  \n",
      "14 -1559.860359  1715.635809 -1256.379688  \n",
      "15   623.570410   998.814017  -591.728533  \n",
      "16   626.987210 -2755.425432 -2073.480065  \n",
      "17 -1181.069576 -1144.403066  3963.626516  \n",
      "18  -659.690762  1613.636730    89.446372  \n",
      "19  5740.687564  4489.401873  1674.215397  \n",
      "20  4793.027129 -4992.924657 -1106.272410  \n",
      "21    27.174924 -3975.992650 -3780.025660  \n",
      "22 -1547.839187  -764.027839  -214.674075  \n",
      "23 -2855.938263   688.129123  -457.379431  \n",
      "24  1872.161117  1123.226899 -1844.507251  \n",
      "25   328.258386  -193.026627 -1519.767592  \n",
      "26  -461.056336   548.338989  -975.243811  \n",
      "27 -5273.671734 -4702.471357  -249.520542  \n",
      "28  1153.697557 -2669.323459  2911.591358  \n",
      "29 -3278.548702 -2293.080398  1301.522218  \n",
      "30  2225.735114  -615.682297 -1638.954236  \n",
      "31  4537.037543 -2350.155210  1240.699354  \n",
      "32 -1025.794828   441.688685  2694.245911  \n",
      "33  -822.194509  3117.588759  -113.431771  \n",
      "34  2125.523745 -1159.321112 -1523.480512  \n",
      "35  -439.152886  -576.777257 -3604.682548  \n",
      "36  1422.494913  5381.528055 -1301.512394  \n",
      "37   421.946757  -393.469146 -2115.744777  \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "TEST\n",
      "              0             1             2             3             4   \\\n",
      "0   -5355.257462   4664.685800   2509.875451   -842.924257   9730.986571   \n",
      "1  -10826.182263   9847.699239   -783.293727   4319.551572   -267.866582   \n",
      "2  -12142.788031  -1719.943887   2677.349819  -1471.615896  -2400.690923   \n",
      "3     194.876978   5280.106064  -3759.861653   2439.499594   7344.430296   \n",
      "4   -5854.369894  -2004.443148   2349.289884   2268.756150   -487.941987   \n",
      "5  -10951.020322   6455.504797   -158.809235   2052.439135   1219.679218   \n",
      "6   -4213.767442   2441.010943   7914.019221  -7992.411574   5209.035652   \n",
      "7   -5692.337511   9513.198997   3883.307546  -3933.232506   4318.235493   \n",
      "8  -10799.356338    463.143050  -1341.478170   4973.306197   2984.854911   \n",
      "9   -6132.247484  10319.860627  -1270.013788   -363.819768   2309.097706   \n",
      "10 -10449.970869   -320.567314  -1264.424042   4270.184885    538.088214   \n",
      "11  29916.623995   4943.372771   6369.144992    978.681887   1646.168318   \n",
      "12  23619.983905 -11714.407475   6552.129916  -3700.351784   6424.668761   \n",
      "13  20123.580398  -1118.326399   3809.502162  11058.545019  -8865.586465   \n",
      "14  25249.295893  -4064.395058  12750.914309  10019.955544   1121.016105   \n",
      "15   -876.167437  -5040.655839  16075.139033  14460.806754  -1603.976694   \n",
      "16 -10349.325480  -6306.493221  -1832.323046   7310.996284  -1379.012430   \n",
      "17 -11557.390818   -646.605242  -2445.646317   6833.364439   1280.382147   \n",
      "18   1574.291466 -10313.565006  23477.603200   7639.212171   1143.902567   \n",
      "19  12192.663314   1569.881900  -1253.385000  18878.122299  -5172.069001   \n",
      "20  -6956.037793   8631.372413  -1238.016784   9971.242868   2524.023521   \n",
      "21  -1441.147796 -14227.976985  17325.903534  17591.594491   2146.732279   \n",
      "22  10966.367423  -6156.362382  -1247.772034   4626.062689  11079.160660   \n",
      "23  20714.741910   3170.784756    667.445201  16292.366892  -9427.138718   \n",
      "24  25841.344298   7186.373352  -1051.160390   9169.514278  -9083.722914   \n",
      "25   -318.637583   1240.668509   9007.016228   4052.059888    395.690222   \n",
      "26  14900.459369   -556.261273   1822.264057   3983.483019   3986.146357   \n",
      "27  -7831.627710  -2512.587869   -557.141529   1412.530951    597.865787   \n",
      "28   4584.306526   3827.710409  -7531.826163    680.809880   3916.526418   \n",
      "29  -4207.814688   5269.822215    358.691985  -1051.104478   1522.158870   \n",
      "30  -2750.499671  10992.873194   -905.281877  -3264.585380   -200.968301   \n",
      "31  -6392.462334  -3759.172384   3370.356650  -2536.060607   1097.198830   \n",
      "32  -1235.908566  11236.819228  -1809.717156  -1948.268487   1338.569604   \n",
      "33  -6836.887782   6011.335760   -631.322024   -369.326158   2618.908597   \n",
      "\n",
      "              5             6            7            8            9   \\\n",
      "0    9380.864319  -7371.261368 -1735.697080  -201.271006  2901.873669   \n",
      "1    1821.850671   2670.978872  3972.526806  1250.222592  2395.060985   \n",
      "2    2953.876873  -1309.331741  3807.779051 -1855.521499   307.721771   \n",
      "3   -5871.250810    960.895989  -662.839412  5428.971440   468.140460   \n",
      "4    -136.340067   2410.760941  7070.376553 -3571.149543   303.620477   \n",
      "5    1088.912680  -1670.618096  4959.107748   186.502514  1074.971518   \n",
      "6    6647.128445  -5449.478575 -1131.616103  -120.204716  4340.154175   \n",
      "7    3728.634193  -1238.106721  2113.005306  2270.279784  5433.894648   \n",
      "8     455.926781  -1631.775737  2018.562060 -1149.623689   643.093491   \n",
      "9    -398.945987    132.617035 -2769.485360  -319.590440 -3747.241043   \n",
      "10   2715.259912  -3629.329973  1566.551199   550.785375     8.289511   \n",
      "11   7852.997493   2826.843434  1005.178679  -493.509493   145.977487   \n",
      "12   7731.185348   2444.323330  9068.033203  5848.589765 -3707.899525   \n",
      "13 -12561.393073  -6237.211071 -1836.280103 -1468.863603  7026.644712   \n",
      "14  -6134.827309     -6.496994 -2197.913201 -3584.434934  1580.028017   \n",
      "15   1701.908259  10636.111999  3693.502522  -202.329740 -3922.518353   \n",
      "16   -465.776435  -1600.716063  1661.331547   -19.563078  1618.967272   \n",
      "17   1302.529741  -1467.375915  2899.804281 -1149.314870  2998.571895   \n",
      "18  -2105.032629   8411.789094  7943.768889  3699.997911 -2386.270860   \n",
      "19   -660.346863   5810.513315  -856.345796   -46.703855  6553.547448   \n",
      "20  -1006.890705   -438.814954  3234.147723  3017.930485   122.131147   \n",
      "21   3489.617313  17389.310665  5286.874007  4476.556162 -7195.121795   \n",
      "22  15611.014762  11173.135940 -4442.587424  1593.040478   478.523555   \n",
      "23  -8421.646045  -1950.333942 -6299.934080 -2750.342980  9036.063281   \n",
      "24   4096.248475   1256.072445 -1630.406338 -3684.732040  1653.920363   \n",
      "25   4158.209204   2997.977023 -1392.270649  -559.257203  1238.028548   \n",
      "26   6695.127908   7142.225863 -1529.877674  -348.297580  2233.163968   \n",
      "27  -1283.585473   -619.869437  1516.368035   279.302038  1881.942211   \n",
      "28  -5693.636138  -2087.420433  -362.959223  2524.172916 -2157.897899   \n",
      "29    450.698546  -1987.872201 -3075.774040  -756.258832 -4915.665782   \n",
      "30  -1388.441376   2420.813815 -1828.473283   805.725775 -3056.771032   \n",
      "31   8760.044635  -5665.217188   485.218164  1128.024185  2421.292706   \n",
      "32   5480.268420   -705.843569  3131.249273  7159.222555  1293.549535   \n",
      "33   2744.109064   -953.026374  1795.427550   -60.922535   227.575303   \n",
      "\n",
      "              10           11           12  \n",
      "0    3030.260434  4456.894881 -1939.547786  \n",
      "1   -3142.541552   175.574698 -1222.743652  \n",
      "2    -370.900230  -984.753964   740.633809  \n",
      "3    4342.782843  2636.509277 -4666.070772  \n",
      "4    1681.274885   925.462261  3242.784740  \n",
      "5    2832.831293  -104.362287  -557.522699  \n",
      "6    3296.426582  2173.876444  1550.172790  \n",
      "7    3459.394591  1306.139812  -606.347524  \n",
      "8     917.579373   228.500418  1016.446782  \n",
      "9     737.434610  2523.051854 -1201.183578  \n",
      "10    741.059170  1359.032836 -3849.566395  \n",
      "11   1033.030145   439.671628  -818.677820  \n",
      "12   -446.210044  1704.294296 -1496.053817  \n",
      "13  11067.783664 -5012.148821  4543.974550  \n",
      "14   4125.488950 -1138.256778   463.574371  \n",
      "15   4498.787150 -5149.829707 -4512.671223  \n",
      "16    -59.891419  1079.359697  2330.436369  \n",
      "17   -894.227425  -212.644783   504.214510  \n",
      "18   4251.597916 -2854.857850  6012.090104  \n",
      "19   8457.040234   492.970425  1885.670817  \n",
      "20   2023.058770  4354.741054   179.346011  \n",
      "21   5064.517549 -2420.340329  2912.669230  \n",
      "22  11871.447128  2489.207805   -51.733936  \n",
      "23   9276.817552 -4219.322580  4924.896233  \n",
      "24   2696.168516 -7434.800797 -1500.345301  \n",
      "25   -398.394580  -425.906306 -6320.686339  \n",
      "26   9251.055653 -7775.661041  1251.124985  \n",
      "27   -587.648687 -3154.344288   421.848135  \n",
      "28   2601.739042 -4157.681701  2056.946362  \n",
      "29   2128.951653  2370.088874  -694.192144  \n",
      "30   1184.581824   -21.219675   286.873871  \n",
      "31   1808.577178   862.479110  -622.184539  \n",
      "32    -84.871681 -2113.619071 -2880.788179  \n",
      "33   1648.583894 -2608.851313  1429.566089  \n"
     ]
    }
   ],
   "source": [
    "scaler=StandardScaler()\n",
    "scaler.fit(trainred)\n",
    "train=scaler.transform(trainred)\n",
    "test=scaler.transform(testred)\n",
    "\n",
    "pca=PCA(.95) \n",
    "pca.fit(trainred)\n",
    "\n",
    "train=pca.transform(trainred)\n",
    "test=pca.transform(testred)\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(pd.DataFrame(train))\n",
    "print(\"\\n\"+\"-\"*50+\"\\n\")\n",
    "print(\"TEST\")\n",
    "print(pd.DataFrame(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning algorithms to compute classifiers\n",
    "\n",
    "Up to this point, we were at the moment able to perform the machine learnings algorithms in order to establish a classification model for the leukaemia diagnosis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear discriminant analysis\n",
    "\n",
    "This model is a generalization of Fisher's linear discriminant and it is used in machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The result is used as a linear classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSSION MATRIX\n",
      "\n",
      "TRAIN\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    11   0\n",
      "1                     0  27\n",
      "\n",
      "TEST\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    13   1\n",
      "1                     0  20\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.95      1.00      0.98        20\n",
      "\n",
      "avg / total       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "AUROC\n",
      "\n",
      "0.9642857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()  # Creating the Linear Discriminant Analysis\n",
    "lda.fit(train, ytrain['cancer']) # Fitting the model\n",
    "ypred_t_lda=lda.predict(train)\n",
    "ypred_lda=lda.predict(test)\n",
    "\n",
    "print(\"CONFUSSION MATRIX\")\n",
    "print(\"\\nTRAIN\\n\")\n",
    "print(pd.crosstab(ytrain['cancer'],ypred_t_lda, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "print(\"\\nTEST\\n\")\n",
    "print(pd.crosstab(ytest['cancer'],ypred_lda, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT\\n\")\n",
    "print(classification_report(ytest['cancer'], ypred_lda))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred_lda)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "This model is a type of supervised machine learning classification algorithm which is used to predict the probability of a categorical dependent variable. The model show a threshold in whom will be specified the result of one of the classes. It is important to point that this algorithm follows Bernoulli Distribution and the final results are shown in a confussion matrix, which evaluates the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSSION MATRIX\n",
      "\n",
      "TRAIN\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    11   0\n",
      "1                     0  27\n",
      "\n",
      "TEST\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    13   1\n",
      "1                     1  19\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93        14\n",
      "          1       0.95      0.95      0.95        20\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "\n",
      "AUROC\n",
      "\n",
      "0.9392857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "rlog = LogisticRegression() # Creando el modelo\n",
    "rlog.fit(train, ytrain['cancer']) \n",
    "ypred_t_rlog = rlog.predict(train) \n",
    "ypred_rlog = rlog.predict(test)\n",
    "\n",
    "print(\"CONFUSSION MATRIX\")\n",
    "print(\"\\nTRAIN\\n\")\n",
    "print(pd.crosstab(ytrain['cancer'],ypred_t_rlog, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "print(\"\\nTEST\\n\")\n",
    "print(pd.crosstab(ytest['cancer'],ypred_rlog, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT\\n\")\n",
    "print(classification_report(ytest['cancer'], ypred_rlog))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred_rlog)\n",
    "roc_auc_rlog = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc_rlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption \n",
    "of conditional independence between every pair of features given the value of the class variable. In this case, we implemented the Gaussian Naive Bayes classifier, which is a special type or Naive Bayes algorithm. It’s specifically used when the features have continuous values.\n",
    "Then, the algorithm creates a classification report that contains the various statistics required to judge a model and a confusion matrix  which gives a clear idea of the accuracy and the fitting of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN\n",
      "\n",
      "\tCONFUSION MATRIX\n",
      "Predicted diagnosis   0   1\n",
      "Expected diagnosis         \n",
      "0                    11   0\n",
      "1                     1  26\n",
      "\n",
      "\n",
      "TEST\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "Predicted diagnosis   0   1\n",
      "Expected diagnosis         \n",
      "0                    13   1\n",
      "1                     3  17\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.93      0.87        14\n",
      "          1       0.94      0.85      0.89        20\n",
      "\n",
      "avg / total       0.89      0.88      0.88        34\n",
      "\n",
      "\n",
      "AUROC\n",
      "\n",
      "0.8892857142857143\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB() # Creating the Naive Bayes model\n",
    "model.fit(train, ytrain['cancer']) # Fitting the model\n",
    "\n",
    "expected_t = ytrain['cancer']\n",
    "ypred_t_by = model.predict(train)  # Making predictions(train)\n",
    "expected = ytest['cancer']\n",
    "ypred_by = model.predict(test)  # Making predictions(test)\n",
    "\n",
    "# Getting Accuracy and Statistics (train)\n",
    "print('\\nTRAIN\\n\\n\\tCONFUSION MATRIX')\n",
    "print(pd.crosstab(expected_t,ypred_t_by, rownames=['Expected diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "\n",
    "# Getting Accuracy and Statistics (test)\n",
    "print(\"\\n\\nTEST\\n\")\n",
    "print('\\nCONFUSION MATRIX')\n",
    "print(pd.crosstab(expected,ypred_by, rownames=['Expected diagnosis'], colnames=['Predicted diagnosis']))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred_by)\n",
    "roc_auc_by = auc(false_positive_rate, true_positive_rate)\n",
    "print('\\nCLASSIFICATION REPORT')\n",
    "print(metrics.classification_report(expected, ypred_by))\n",
    "print(\"\\nAUROC\\n\")  \n",
    "print(roc_auc_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fits properly to the training data. As it can be seen in the confusion matrix, the prediction was correct in 37 out of 38 patients. However, at the time of making the prediction with the test data, the model manages to make fewer correct predictions, since it only adequately diagnoses 30 of 34 patients. Even so, our model is quite accurate classifying the categories of our dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine¶\n",
    "This model is a type of supervised machine learning classification algorithm. The algorithm chooses the most optimal decision boundary (a region which maximizes the distance between the nearest data of all the classes), which is the one that has the maximum margin from the nearest points of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSSION MATRIX\n",
      "\n",
      "TRAIN\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    11   0\n",
      "1                     0  27\n",
      "\n",
      "TEST\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    13   1\n",
      "1                     0  20\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.95      1.00      0.98        20\n",
      "\n",
      "avg / total       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "AUROC\n",
      "\n",
      "0.9642857142857143\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(train, ytrain['cancer'])\n",
    "ypred_svm = svclassifier.predict(test)\n",
    "ypred_t_svm=svclassifier.predict(train)\n",
    "\n",
    "print(\"CONFUSSION MATRIX\")\n",
    "print(\"\\nTRAIN\\n\")\n",
    "print(pd.crosstab(ytrain['cancer'],ypred_t_svm, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "print(\"\\nTEST\\n\")\n",
    "print(pd.crosstab(ytest['cancer'],ypred_svm, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT\\n\")\n",
    "print(classification_report(ytest['cancer'], ypred_svm))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred_svm)\n",
    "roc_auc_svm = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network \n",
    "This classification model is based in the biological neurons. It receives an input and produces a signal (output) based on it which is received by another neuron as a new input. Each neuron has an activation function, which determines whether an output is computed for a determined input will be sended to another neuron or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    10   1\n",
      "1                     1  26\n",
      "CONFUSSION MATRIX\n",
      "\n",
      "TEST\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    10   4\n",
      "1                     3  17\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.71      0.74        14\n",
      "          1       0.81      0.85      0.83        20\n",
      "\n",
      "avg / total       0.79      0.79      0.79        34\n",
      "\n",
      "\n",
      "AUROC\n",
      "\n",
      "0.7821428571428573\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp.fit(train, ytrain['cancer'])\n",
    "ypred_nn=mlp.predict(test)\n",
    "\n",
    "ypred_t_nn=mlp.predict(train)\n",
    "print(\"\\nTRAIN\\n\")\n",
    "print(pd.crosstab(ytrain['cancer'],ypred_t_nn, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "print(\"CONFUSSION MATRIX\")\n",
    "print(\"\\nTEST\\n\")\n",
    "print(pd.crosstab(ytest['cancer'],ypred_nn, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT\\n\")\n",
    "print(classification_report(ytest['cancer'], ypred_nn))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred_nn)\n",
    "roc_auc_nn = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "\n",
    "This computes a diagram with logic construction in order to represent a set of conditions, which are consecutively assessed for the resolution of a problem. In order to get the best results, the parameters to be used for building the model where calculated using a cross-validation algorithm. The best parameters were used to build the final decision tree, which was evaluated with the test sample, constructing the confussion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n",
      "{'min_samples_split': 2, 'max_depth': 2}\n",
      "CONFUSSION MATRIX\n",
      "\n",
      "TRAIN\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    11   0\n",
      "1                     0  27\n",
      "\n",
      "TEST\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    11   3\n",
      "1                     1  19\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.79      0.85        14\n",
      "          1       0.86      0.95      0.90        20\n",
      "\n",
      "avg / total       0.89      0.88      0.88        34\n",
      "\n",
      "\n",
      "AUROC\n",
      "\n",
      "0.8678571428571428\n"
     ]
    }
   ],
   "source": [
    "split_range=list(range(2,15))\n",
    "prof_range=list(range(2, 10))\n",
    "\n",
    "param_grid={'min_samples_split':split_range, 'max_depth':prof_range}\n",
    "clf_gini = DecisionTreeClassifier(criterion = 'gini', random_state=100)\n",
    "grid_dt=GridSearchCV(clf_gini, param_grid, scoring='accuracy')\n",
    "grid_dt.fit(train, ytrain['cancer'])\n",
    "print(grid_dt.best_score_)\n",
    "print(grid_dt.best_params_)\n",
    "\n",
    "mejor_clf_gini=grid_dt.best_estimator_\n",
    "\n",
    "\n",
    "ypred=mejor_clf_gini.predict(test)\n",
    "ypred_t=mejor_clf_gini.predict(train)\n",
    "\n",
    "print(\"CONFUSSION MATRIX\")\n",
    "print(\"\\nTRAIN\\n\")\n",
    "print(pd.crosstab(ytrain['cancer'],ypred_t, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "print(\"\\nTEST\\n\")\n",
    "print(pd.crosstab(ytest['cancer'],ypred, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT\\n\")\n",
    "print(classification_report(ytest['cancer'], ypred))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated decision tree is not a bad classifier, but there is some kind of overfitting. When we apply this model to the training set, the accuracy of it is perfect: every patient is correctly diagnose. However, when running the classifier model for the test set, 3 patients are misdiagnosed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Instead of using a single decision tree, this algorithm computes a whole forest of trees with little depth. In order to obtain the classification result, it takes the individual result of each tree and the resulting class is the most \"voted\" one. \n",
    "In order to tackle the aforementioned overfitting, a cross-validation search of the best hypeparameters was also performed. \n",
    "Due to the high number of hyperparameters to be tested, two tipes of parameter search are computed. In first place, a random search is perform in order to get close to the actual best value of each parameter. Once we have some idea of this approximate value, we perform the grid search by building a grid within a range including this approximation. Best computed parameters are finally used to compute the final ranfom forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 200, 'max_features': 'sqrt', 'min_samples_split': 2, 'max_depth': 110}\n",
      "0.9473684210526315\n",
      "{'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 1000, 'min_samples_split': 6, 'max_features': 'auto', 'max_depth': None}\n",
      "CONFUSSION MATRIX\n",
      "\n",
      "TRAIN\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    11   0\n",
      "1                     0  27\n",
      "\n",
      "TEST\n",
      "\n",
      "Predicted diagnosis   0   1\n",
      "Actual diagnosis           \n",
      "0                    12   2\n",
      "1                     0  20\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        14\n",
      "          1       0.91      1.00      0.95        20\n",
      "\n",
      "avg / total       0.95      0.94      0.94        34\n",
      "\n",
      "\n",
      "AUROC\n",
      "\n",
      "0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_clf=RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf_clf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=100, n_jobs = -1)\n",
    "rf_random.fit(train, ytrain['cancer'])\n",
    "\n",
    "\n",
    "\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [None],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'n_estimators': [500, 1000, 2000]\n",
    "}\n",
    "\n",
    "grid_rf=GridSearchCV(rf_clf, param_grid, scoring='accuracy', cv=3)\n",
    "grid_rf.fit(train, ytrain['cancer'])\n",
    "print(grid_rf.best_score_)\n",
    "print(grid_rf.best_params_)\n",
    "\n",
    "mejor_rf_clf=grid_rf.best_estimator_\n",
    "\n",
    "\n",
    "ypred_rf=mejor_rf_clf.predict(test)\n",
    "ypred_t_rf=mejor_rf_clf.predict(train)\n",
    "\n",
    "print(\"CONFUSSION MATRIX\")\n",
    "print(\"\\nTRAIN\\n\")\n",
    "print(pd.crosstab(ytrain['cancer'],ypred_t_rf, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "print(\"\\nTEST\\n\")\n",
    "print(pd.crosstab(ytest['cancer'],ypred_rf, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT\\n\")\n",
    "print(classification_report(ytest['cancer'], ypred_rf))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred_rf)\n",
    "roc_auc_rf = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and conclusion\n",
    "Using different machine learning (ML) tools, we have been able to obtain some models that certainly could be useful when differentially diagnosis these two types of cancer. The best results have been achieved when Linear Discriminant Analysis and Support Vector Machine models were used, with which default parameters were set. This lead us to the conclusion that sometimes the easiest approach is also the most convinient.\n",
    "On the other hand, poor results were got when a neural netwok model was used. This could be due to the high complexity of this kind of models, in which many parameters can be set in order to get to the best results. \n",
    "Finally, best parameter search was performed in decision tree and random forest models, just in order to get to know the tools commonly used for this aim. We found that this is a useful but time-consuming and computer demanding technique.\n",
    "On the whole, we have proved that ML analysis consist in an interesting tool with a vast range of application in precission medicine. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
