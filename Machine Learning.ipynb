{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatics Project\n",
    "\n",
    "## Preliminar data analysis\n",
    "\n",
    "Our available data for the performance of prediction models consists in two dataframes. The first one, already labelled as training data, contains 38 observations of patients, to whom the expression of 7129 genes has been assessed in order to identify any characteristic expression pattern for differential diagnosis of two leucemia conditios: acute myeloid leukemia (AML) and acute lymphoid leukemia (ALL). The test data (in which the classification models are going to be tested) consist in the expression levels of the same genes for 34 different genes. The actual diagnosis of both groups is also provided.\n",
    "\n",
    "## Data pre-processing\n",
    "Due to the vast extension of the data, an initial pre-procesing is needed in order to minimize the number of features to be used with the ML tools.\n",
    "Since the expression levels of many genes has been analysed, it is important to determine which of them show a correlation with the differential diagnosis of the diseases (i.e. which of them show a a significant change in the expression when the patient has been diagnosed with ALL or AML).In order to do that, it is useful t perfom a feature selection.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maria\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#Importing all the necessary libraries for the ML analysis\n",
    "#import scipy\n",
    "import numpy as np\n",
    "#import matplotlib\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "##Importar los metodos que vamos a utilizar\n",
    "from sklearn.metrics import classification_report\n",
    "#from pandas.plotting import scatter_matrix\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the files:\n",
    "\n",
    "#These paths might change according to the locations of the files within one's computer\n",
    "ruta_train='C:\\\\Users\\\\Maria\\\\OneDrive\\\\Master\\\\Bioinformatica\\\\Trabajo\\\\Originales\\\\data_set_ALL_AML_train.csv'\n",
    "ruta_y='C:\\\\Users\\\\Maria\\\\OneDrive\\\\Master\\\\Bioinformatica\\\\Trabajo\\\\Originales\\\\actual.csv'\n",
    "ruta_test='C:\\\\Users\\\\Maria\\\\OneDrive\\\\Master\\\\Bioinformatica\\\\Trabajo\\\\Originales\\\\data_set_ALL_AML_independent.csv'\n",
    "train=pd.read_csv(ruta_train)\n",
    "test=pd.read_csv(ruta_test)\n",
    "y=pd.read_csv(ruta_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are initially placed in the rows of the dataframe, while patients are in the columns. Thus, it is necessary to transpose the data. Besides, there are some columns name 'call', between the patients information, whose data is not relevant for our model. These columns were also removed. Lastly, the name of the features was changed and the actual expression measure was converted into a numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transposing the data\n",
    "train=train.T\n",
    "test=test.T\n",
    "\n",
    "\n",
    "#Removing call data\n",
    "for fila in train.index:\n",
    "    if 'call' in fila:\n",
    "        train=train.drop(fila)\n",
    "\n",
    "for fila in test.index:\n",
    "    if 'call' in fila:\n",
    "        test=test.drop(fila)\n",
    "\n",
    "#Collumns are labelled with the gene accession number\n",
    "\n",
    "columnastrain=train.loc['Gene Accession Number']\n",
    "train=train[2:]\n",
    "train.columns=columnastrain\n",
    "\n",
    "columnastest=test.loc['Gene Accession Number']\n",
    "test=test[2:]\n",
    "test.columns=columnastest\n",
    "\n",
    "#Converting into numeric\n",
    "train=train.astype('float')\n",
    "test=test.astype('float')\n",
    "train.index=train.index.astype(int)\n",
    "test.index=test.index.astype(int)\n",
    "train=train.sort_index()\n",
    "test=test.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is coded as a categorical variable with 1 coding ALL and 0 coding AML. This response is split into the diagnosis of test and training patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.index=y['patient']\n",
    "y['cancer']=np.where(y.cancer=='ALL', 1, 0) \n",
    "y.groupby('cancer').size() \n",
    "ytrain=y[:len(train)]\n",
    "ytest=y[len(train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has the accurate structure, it is time to perform the aforementioned feature selection. Two algorithms for this aim were used in a combined way. In the first one, an univariant selection was perform, and thus each feature was individually selected or removed for the final analysis. In order to do that, the Fischer score was computed and the 100 best atributes were selected. \n",
    "On the other hand, a packing selection tool was used. This type of tools consider the selection as a searching problem (typical of the artificial intelligence tools), and different combinations of features are evaluated and compared. To each of these combinations, a score is computed and assigned and some algorithms are run to select the best combinations. In this case, the algorithm for the selection was the recursive feature removal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [38, 72]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f739c6e68099>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcolumnas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mseleccionadas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_classif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cancer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0matrib\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseleccionadas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0matributos1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumnas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Maria\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \"\"\"\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Maria\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Maria\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [38, 72]"
     ]
    }
   ],
   "source": [
    "#Univariant feature selection (F-score)\n",
    "k = 100  \n",
    "columnas = list(train.columns.values)\n",
    "seleccionadas = SelectKBest(f_classif, k=k).fit(train, ytrain['cancer'])\n",
    "atrib = seleccionadas.get_support()\n",
    "atributos1 = [columnas[i] for i in list(atrib.nonzero()[0])]\n",
    "\n",
    "#Recursive atribute selecion\n",
    "modelo = ExtraTreesClassifier()\n",
    "era = RFE(modelo, 100)  # número de atributos a seleccionar\n",
    "era = era.fit(train, ytrain['cancer'])\n",
    "atrib2 = era.support_\n",
    "atributos2 = [columnas[i] for i in list(atrib2.nonzero()[0])]\n",
    "\n",
    "#Se combinan los tributos elegidos en una lista\n",
    "print('The selected features are: ')\n",
    "atribselec=list(set(atributos1)|set(atributos2))\n",
    "print(atribselec)\n",
    "\n",
    "trainred=pd.DataFrame()\n",
    "for i in atribselec:\n",
    "    trainred[i]=train[i]\n",
    "\n",
    "testred=pd.DataFrame()\n",
    "for i in atribselec:\n",
    "    testred[i]=test[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 100 features have been selected with each type of tool, a maximum number of 200 were considered to be significant for our classification models. They may also be too many, and so a principal component analysis (PCA) was performed. This is an statistical tool used for describing complex data in terms of new uncorrelationated variables. In this case, the algorithm is computed in a way that the new variables mantain the 95% of the variance in the original variables. Since PCA is affected by the data scale, it is necessary to standarize it previously to the computing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "scaler.fit(trainred)\n",
    "train=scaler.transform(trainred)\n",
    "test=scaler.transform(testred)\n",
    "\n",
    "pca=PCA(.95) \n",
    "pca.fit(trainred)\n",
    "\n",
    "train=pca.transform(trainred)\n",
    "test=pca.transform(testred)\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(pd.DataFrame(train))\n",
    "print(\"\\n\"+\"-\"*50+\"\\n\")\n",
    "print(\"TEST\")\n",
    "print(pd.DataFrame(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning algorithms to compute classifiers\n",
    "\n",
    "Up to this point, we are now able to perform the machine learnings algorithms in order to establish a classification model for the leukemia diagnosis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "This model is used to estimate the real values based on continuous variables. The idea is to try to establish the relationship between the independent variables (x = features) and dependent (y = type of cancer) by adjusting a line in relation to the training values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = LinearRegression() # Creating the LinearRegression model\n",
    "rl.fit(train, ytrain['cancer']) # Adjusting the model\n",
    "ypred_rl = rl.predict(test) # Making predictions\n",
    "print(f'\\n\\tPREDICTED VALUES\\n {ypred_rl}') # Disease prediction values of the 34 patients\n",
    "print(f'\\n\\tDESVIATION')\n",
    "print(np.mean(expected - predicted_rl)) # Computing the desviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption \n",
    "of conditional independence between every pair of features given the value of the class variable. In this case, we have implemented the\n",
    "Gaussian Naive Bayes classifier, which is a especial type o Naive Bayes algorithm. It’s specifically used when the features have continuous values.\n",
    "Then, the algorithm creates a classification report that contains the various statistics required to judge a model and a confusion matrix \n",
    "which will give us a clear idea of the accuracy and the fitting of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB() # Creating the Naive Bayes model\n",
    "model.fit(train, ytrain['cancer']) # Fitting the model\n",
    "\n",
    "expected = ytrain['cancer']\n",
    "ypred_t_by = model.predict(train)  # Making predictions(train)\n",
    "\n",
    "# Getting Accuracy and Statistics (train)\n",
    "print(f'\\nTRAIN\\n\\n\\tCONFUSION MATRIX')\n",
    "print(pd.crosstab(expected,ypred_t_by, rownames=['Expected diagnosis'], colnames=['Predicted diagnosis']))\n",
    "print(f'\\n\\tACCURACY')\n",
    "print(accuracy_score(expected, ypred_t_by, normalize = True))\n",
    "\n",
    "expected = ytest['cancer']\n",
    "ypred_by = model.predict(test)  # Making predictions(test)\n",
    "\n",
    "# Getting Accuracy and Statistics (test)\n",
    "print(f'\\n\\nTEST\\n\\n\\tCLASSIFICATION REPORT')\n",
    "print(metrics.classification_report(expected, ypred_by))\n",
    "print(f'\\n\\tCONFUSION MATRIX')\n",
    "print(pd.crosstab(expected,ypred_by, rownames=['Expected diagnosis'], colnames=['Predicted diagnosis']))\n",
    "print(f'\\n\\tACCURACY')\n",
    "print(accuracy_score(expected, ypred_by, normalize = True))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred_by)\n",
    "roc_auc_by = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is very adjusted to the training data, providing a 97% reliability. As it can appreciate in the confusion matrix, the prediction was correct in 37 out of 38 patients. However, at the time of making the prediction with the test data, the model manages to make fewer correct predictions, since it only adequately diagnoses 30 of 34 patients. Even so, our model is quite accurate classifying the categories of our dataset.\n",
    "\n",
    "### Decision tree\n",
    "The second implemented algorithm was the decision tree classifier. which computes a diagram with logic construction in order to represent a set of conditions, which are consecutively assessed for the resolution of a problem. In order to get the best results, the parameters to be used for building the model where calculated using a cross-validation algorithm. The best parameters were used to build the final decision tree, which was evaluated with the test sample, constructing the confussion matrix. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_range=list(range(2,15))\n",
    "prof_range=list(range(2, 10))\n",
    "\n",
    "param_grid={'min_samples_split':split_range, 'max_depth':prof_range}\n",
    "clf_gini = DecisionTreeClassifier(criterion = 'gini', random_state=100)\n",
    "grid_dt=GridSearchCV(clf_gini, param_grid, scoring='accuracy')\n",
    "grid_dt.fit(train, ytrain['cancer'])\n",
    "print(grid_dt.best_score_)\n",
    "print(grid_dt.best_params_)\n",
    "\n",
    "mejor_clf_gini=grid_dt.best_estimator_\n",
    "\n",
    "\n",
    "ypred=mejor_clf_gini.predict(test)\n",
    "#print(mejor_clf_gini.predict_proba(test))\n",
    "print(\"CONFUSSION MATRIX\")\n",
    "print(\"\\nTEST\\n\")\n",
    "print(pd.crosstab(ytest['cancer'],ypred, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "ypred_t=mejor_clf_gini.predict(train)\n",
    "print(\"\\nTRAIN\\n\")\n",
    "print(pd.crosstab(ytrain['cancer'],ypred_t, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT\\n\")\n",
    "print(classification_report(ytest['cancer'], ypred))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated decision tree is not a bad classifier, but there is some kind of overfitting. When we apply this model to the training set, the accuracy of it is perfect: every patient is correctly diagnose. However, when running the classifier model for the test set, 3 patients are misdiagnosed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Instead of using a single decision tree, this algorithm computes a whole forest of trees with little depth. In order to obtain the classification result, it takes the individual result of each tree and the resulting class is the most \"voted\" one. \n",
    "In order to tackle the aforementioned overfitting, a cross-validation search of the best hypeparameters was also performed. \n",
    "Due to the high number of hyperparameters to be tested, two tipes of parameter search are computed. In first place, a random search is perfomr in order to get close to the actual best value of each parameter. Once we have some idea of this approximate value, we perform the grid search by building a grid within a range including this approximation. Best computed parameters are finally used to compute the final ranfom forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_clf=RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf_clf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=100, n_jobs = -1)\n",
    "rf_random.fit(train, ytrain['cancer'])\n",
    "\n",
    "\n",
    "\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [None],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'n_estimators': [500, 1000, 2000]\n",
    "}\n",
    "\n",
    "grid_rf=GridSearchCV(rf_clf, param_grid, scoring='accuracy', cv=3)\n",
    "grid_rf.fit(train, ytrain['cancer'])\n",
    "print(grid_rf.best_score_)\n",
    "print(grid_rf.best_params_)\n",
    "\n",
    "mejor_rf_clf=grid_rf.best_estimator_\n",
    "\n",
    "\n",
    "ypred_rf=mejor_rf_clf.predict(test)\n",
    "\n",
    "print(\"CONFUSSION MATRIX\")\n",
    "print(\"\\nTEST\\n\")\n",
    "print(pd.crosstab(ytest['cancer'],ypred_rf, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "ypred_t_rf=mejor_rf_clf.predict(train)\n",
    "print(\"\\nTRAIN\\n\")\n",
    "print(pd.crosstab(ytrain['cancer'],ypred_t_rf, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT\\n\")\n",
    "print(classification_report(ytest['cancer'], ypred_rf))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred_rf)\n",
    "roc_auc_rf = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(train, ytrain['cancer'])\n",
    "y_pred_svm = svclassifier.predict(test)\n",
    "ypred_t_svm=svclassifier.predict(train)\n",
    "\n",
    "print(\"CONFUSSION MATRIX\")\n",
    "print(\"\\nTEST\\n\")\n",
    "print(pd.crosstab(ytest['cancer'],ypred_svm, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "print(\"\\nTRAIN\\n\")\n",
    "print(pd.crosstab(ytrain['cancer'],ypred_t_svm, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT\\n\")\n",
    "print(classification_report(ytest['cancer'], ypred_svm))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred_svm)\n",
    "roc_auc_svm = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network \n",
    "This calssification model is based in the biological neurons. It recieves an input and produces a signal (output) based on it wich is recived by another neuron as a new input. Each neuron has an activation function, which determines whether an output is computed for a determined input will be sended to another neuron or not. Due to the complexity of this algorithm, in this case no parameter search was computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))\n",
    "mlp.fit(train, ytrain['cancer'])\n",
    "ypred_nn=mlp.predict(test)\n",
    "\n",
    "print(\"CONFUSSION MATRIX\")\n",
    "print(\"\\nTEST\\n\")\n",
    "print(pd.crosstab(ytest['cancer'],ypred_nn, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "ypred_t_nn=mlp.predict(train)\n",
    "print(\"\\nTRAIN\\n\")\n",
    "print(pd.crosstab(ytrain['cancer'],ypred_t_nn, rownames=['Actual diagnosis'], colnames=['Predicted diagnosis']))\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT\\n\")\n",
    "print(classification_report(ytest['cancer'], ypred_nn))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(ytest['cancer'], ypred_rf)\n",
    "roc_auc_nn = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUROC\\n\")\n",
    "print(roc_auc_nn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
